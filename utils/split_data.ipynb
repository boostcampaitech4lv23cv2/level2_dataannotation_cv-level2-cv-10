{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212e4358-c6e9-427c-b4eb-7ad62637189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1095705b-5a6e-4c2e-9662-b8f48de92e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "seed = 444\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47efe2a5-abb6-4254-8b71-ab8e32f54210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'points': [[662.0, 747.0],\n",
       "   [945.0, 759.0],\n",
       "   [922.0, 1582.0],\n",
       "   [673.0, 1565.0]],\n",
       "  'transcription': '출입금지',\n",
       "  'language': ['ko'],\n",
       "  'illegibility': False,\n",
       "  'orientation': 'Horizontal',\n",
       "  'word_tags': None},\n",
       " '1': {'points': [[476.0, 551.0],\n",
       "   [1132.0, 554.0],\n",
       "   [1118.0, 747.0],\n",
       "   [471.0, 716.0]],\n",
       "  'transcription': '오토바이',\n",
       "  'language': ['ko'],\n",
       "  'illegibility': False,\n",
       "  'orientation': 'Horizontal',\n",
       "  'word_tags': None},\n",
       " '2': {'points': [[455.0, 293.0],\n",
       "   [1144.0, 310.0],\n",
       "   [1129.0, 518.0],\n",
       "   [457.0, 496.0]],\n",
       "  'transcription': '자전거',\n",
       "  'language': ['ko'],\n",
       "  'illegibility': False,\n",
       "  'orientation': 'Horizontal',\n",
       "  'word_tags': None}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_json(filename):\n",
    "    with Path(filename).open(encoding='utf8') as handle:\n",
    "        ann = json.load(handle)\n",
    "    return ann\n",
    "\n",
    "def save_json(data: dict, file_nm: str, dir_path: str):\n",
    "    with open(os.path.join(dir_path, file_nm), 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "root_dir = '/opt/ml/input/data/ICDAR17_Ko_UP'\n",
    "\n",
    "data_org = read_json(os.path.join(root_dir, 'ufo/train.json'))\n",
    "\n",
    "data_org['images']['img_4380.jpg']['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c530d85-b9f4-4c4a-80a0-093b49834c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_size(quads):\n",
    "    \"\"\" 단어 영역의 사각형 좌표가 주어졌을 때 가로, 세로길이를 계산해주는 함수.\n",
    "    TODO: 각 변의 길이를 단순히 max로 처리하기때문에 직사각형에 가까운 형태가 아니면 약간 왜곡이 있다.\n",
    "    Args:\n",
    "        quads: np.ndarray(n, 4, 2) n개 단어 bounding-box의 4개 점 좌표 (단위 pixel)\n",
    "    Return:\n",
    "        sizes: np.ndarray(n, 2) n개 box의 (height, width)쌍\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    for i, j in [(1, 2), (3, 0), (0, 1), (2, 3)]: # [right(height), left(height), upper(width), lower(width)] sides\n",
    "        dists.append(np.linalg.norm(quads[:, i] - quads[:, j], ord=2, axis=1))\n",
    "\n",
    "    dists = np.stack(dists, axis=-1).reshape(-1, 2, 2) # shape (n, 2, 2) widths, heights into separate dim\n",
    "    return np.rint(dists.mean(axis=-1)).astype(int)\n",
    "\n",
    "\n",
    "def rectify_poly(poly, direction, img_w, img_h):\n",
    "    \"\"\"일반 polygon형태인 라벨을 크롭하고 rectify해주는 함수.\n",
    "    Args:\n",
    "        poly: np.ndarray(2n+4, 2) (where n>0), 4, 6, 8\n",
    "        image: np.ndarray opencv 포멧의 이미지\n",
    "        direction: 글자의 읽는 방향과 진행 방향의 수평(Horizontal) 혹은 수직(Vertical) 여부\n",
    "    Return:\n",
    "        rectified: np.ndarray(2, ?) rectify된 단어 bbox의 사이즈.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_pts = poly.shape[0]\n",
    "    assert n_pts % 2 == 0\n",
    "    if n_pts == 4:\n",
    "        size = get_box_size(poly[None])\n",
    "        h = size[:, 0] / img_h\n",
    "        w = size[:, 1] / img_w\n",
    "        return np.stack((h,w))\n",
    "\n",
    "    def unroll(indices):\n",
    "        return list(zip(indices[:-1], indices[1:]))\n",
    "\n",
    "    # polygon하나를 인접한 사각형 여러개로 쪼갠다.\n",
    "    indices = list(range(n_pts))\n",
    "    if direction == 'Horizontal':\n",
    "        upper_pts = unroll(indices[:n_pts // 2]) # (0, 1), (1, 2), ... (4, 5)\n",
    "        lower_pts = unroll(indices[n_pts // 2:])[::-1] # (8, 9), (7, 8), ... (6, 7)\n",
    "\n",
    "        quads = np.stack([poly[[i, j, k, l]] for (i, j), (k, l) in zip(upper_pts, lower_pts)])\n",
    "    else:\n",
    "        right_pts = unroll(indices[1:n_pts // 2 + 1]) # (1, 2), (2, 3), ... (4, 5)\n",
    "        left_pts = unroll([0] + indices[:n_pts // 2:-1]) # (0, 9), (9, 8), ... (7, 6)\n",
    "\n",
    "        quads = np.stack([poly[[i, j, k, l]] for (j, k), (i, l) in zip(right_pts, left_pts)])\n",
    "\n",
    "    sizes = get_box_size(quads)\n",
    "    if direction == 'Horizontal':\n",
    "        h = sizes[:, 0].max() / img_h\n",
    "        widths = sizes[:, 1]\n",
    "        w = np.sum(widths) / img_w\n",
    "        return np.stack((h,w)).reshape(2,-1)\n",
    "        #return np.stack((h,w))\n",
    "    elif direction == 'Vertical':\n",
    "        heights = sizes[:, 0]\n",
    "        w = sizes[:, 1].max() / img_w\n",
    "        h = np.sum(heights) / img_h\n",
    "        return np.stack((h,w)).reshape(2,-1)\n",
    "    else:\n",
    "        h = sizes[:, 0] / img_h\n",
    "        w = sizes[:, 1] / img_w\n",
    "        return np.stack((h,w),-1)\n",
    "    \n",
    "def get_image_dfs(data):\n",
    "    df = {}\n",
    "    df['image'] = []\n",
    "    df['word_counts'] = []\n",
    "    df['image_width'] = []\n",
    "    df['image_height'] = []\n",
    "    df['image_tags'] = []\n",
    "    img_tags = []\n",
    "\n",
    "    quads = []\n",
    "    polys = []\n",
    "    seq_length = []\n",
    "    hor_sizes = []\n",
    "    ver_sizes = []\n",
    "    irr_sizes = []\n",
    "    languages = []\n",
    "    orientation = []\n",
    "    word_tags = []\n",
    "    aspect_ratio = []\n",
    "    ver_string = []\n",
    "\n",
    "    bbox_properties = []\n",
    "    \n",
    "    for image_key, image_value in data[\"images\"].items():\n",
    "        df['image'].append(image_key)\n",
    "        img_w = image_value['img_w']\n",
    "        img_h = image_value['img_h']\n",
    "        df['image_width'].append(img_w)\n",
    "        df['image_height'].append(img_h)\n",
    "        df['image_tags'].append(image_value['tags'])\n",
    "        df['image_tags']= [['None'] if v is None else v for v in df['image_tags']] # our data does not inlcude multi-tag images \n",
    "        word_ann = image_value['words']\n",
    "        count_ill = 0 \n",
    "        for word in word_ann.values():\n",
    "            if word['illegibility']== False:\n",
    "                orientation.append(word['orientation'])\n",
    "                orientation = [v for v in orientation]\n",
    "                seq_length.append(len(word['transcription']))\n",
    "                languages.append(word['language'])\n",
    "                languages = [['None'] if v is None else v for v in languages] # our data does not inlcude multi-language words\n",
    "                if word['word_tags'] != None:\n",
    "                    word_tags.extend(word['word_tags'][:])\n",
    "                elif word['word_tags']== None:\n",
    "                    word_tags.append('None')\n",
    "                poly = np.int32(word['points'])\n",
    "                size = rectify_poly(poly, word['orientation'], img_w, img_h)\n",
    "                if word['orientation'] == 'Horizontal':\n",
    "                    hor_sizes.append(size)\n",
    "                    bbox_properties.append([image_key, size, 'Horizontal'])\n",
    "                elif word['orientation'] == 'Vertical':\n",
    "                    ver_sizes.append(size)\n",
    "                    bbox_properties.append([image_key, size, 'Vertical'])\n",
    "                else:\n",
    "                    irr_sizes.append(size)\n",
    "                    bbox_properties.append([image_key, size, 'Irregular'])\n",
    "            else:\n",
    "                count_ill += 1\n",
    "\n",
    "        df['word_counts'].append(len(word_ann)-count_ill)\n",
    "\n",
    "\n",
    "    all_sizes = hor_sizes + ver_sizes + irr_sizes\n",
    "    quad_area = [all_sizes[i][0]*all_sizes[i][1] for i in range(len(all_sizes))]\n",
    "    total_area = []\n",
    "    for s in quad_area:\n",
    "        if s.shape[0] == 1:\n",
    "            total_area.append(np.sum(s[0])) \n",
    "        else:\n",
    "            total_area.append(np.sum(s))\n",
    "\n",
    "    hor_aspect_ratio = [hor_sizes[i][1]/hor_sizes[i][0] for i in range(len(hor_sizes))]\n",
    "    ver_aspect_ratio = [ver_sizes[i][1]/ver_sizes[i][0] for i in range(len(ver_sizes))]\n",
    "\n",
    "    image_df = pd.DataFrame.from_dict(df)\n",
    "    bbox_df = pd.DataFrame(data=bbox_properties,\n",
    "                          columns=['image', 'size', 'orientation'])\n",
    "    \n",
    "    bbox_df['aspect_ratio'] = bbox_df.apply(lambda x: (x['size'][1]/x['size'][0])[0], axis=1)\n",
    "    \n",
    "    return image_df, bbox_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196359c5-c49a-4f6f-bb25-b82caae9916d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_df, bbox_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_org\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 108\u001b[0m, in \u001b[0;36mget_image_dfs\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    106\u001b[0m languages\u001b[38;5;241m.\u001b[39mappend(word[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m languages \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m languages] \u001b[38;5;66;03m# our data does not inlcude multi-language words\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     word_tags\u001b[38;5;241m.\u001b[39mextend(word[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_tags\u001b[39m\u001b[38;5;124m'\u001b[39m][:])\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m word[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tags'"
     ]
    }
   ],
   "source": [
    "image_df, bbox_df = get_image_dfs(data_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd018ceb-319e-4093-a5df-a30ef991ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03296c68-699e-4c91-ab3e-e066fb66902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede345a6-6c02-4550-b705-0e51f6660d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random split\n",
    "X_train, X_test, _ = train_test_split(image_df.image, image_df.image, test_size=0.2, shuffle=True, random_state=seed)\n",
    "\n",
    "train = {'images': {k: v for k, v in data_org['images'].items() if k in X_train.values}}\n",
    "valid = {'images': {k: v for k, v in data_org['images'].items() if k in X_test.values}}\n",
    "\n",
    "data_list = [train, valid]\n",
    "file_nm_list = ['train1.json', 'valid1.json']\n",
    "\n",
    "for data, file_nm in zip(data_list, file_nm_list):\n",
    "    save_json(data, file_nm, dir_path=os.path.join(root_dir, path_ICDAR17))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Nov  4 2020, 21:23:28) \n[Clang 12.0.0 (clang-1200.0.32.28)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
